{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expressions & word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_string = str([\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', ''])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([#|@]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÃœ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple topic identification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = [(6, 1),\n",
    " (13, 3),\n",
    " (19, 3),\n",
    " (25, 2),\n",
    " (26, 1),\n",
    " (27, 1),\n",
    " (33, 22),\n",
    " (34, 3),\n",
    " (36, 1),\n",
    " (38, 1),\n",
    " (39, 1),\n",
    " (40, 1),\n",
    " (42, 7),\n",
    " (46, 2),\n",
    " (47, 2),\n",
    " (48, 12),\n",
    " (50, 1),\n",
    " (55, 5),\n",
    " (56, 3),\n",
    " (60, 22),\n",
    " (63, 91),\n",
    " (68, 1),\n",
    " (69, 1),\n",
    " (71, 5),\n",
    " (72, 1),\n",
    " (75, 1),\n",
    " (79, 2),\n",
    " (82, 3),\n",
    " (83, 1),\n",
    " (87, 4),\n",
    " (90, 1),\n",
    " (92, 2),\n",
    " (95, 2),\n",
    " (100, 3),\n",
    " (103, 1),\n",
    " (110, 9),\n",
    " (113, 1),\n",
    " (114, 1),\n",
    " (116, 7),\n",
    " (118, 3),\n",
    " (119, 3),\n",
    " (122, 4),\n",
    " (128, 1),\n",
    " (131, 3),\n",
    " (139, 5),\n",
    " (142, 4),\n",
    " (148, 1),\n",
    " (150, 2),\n",
    " (152, 5),\n",
    " (161, 1),\n",
    " (162, 51),\n",
    " (163, 8),\n",
    " (165, 13),\n",
    " (166, 26),\n",
    " (170, 5),\n",
    " (176, 2),\n",
    " (186, 14),\n",
    " (193, 1),\n",
    " (194, 2),\n",
    " (196, 6),\n",
    " (199, 5),\n",
    " (200, 3),\n",
    " (202, 3),\n",
    " (206, 2),\n",
    " (209, 1),\n",
    " (210, 4),\n",
    " (214, 1),\n",
    " (215, 1),\n",
    " (220, 1),\n",
    " (224, 3),\n",
    " (226, 3),\n",
    " (231, 1),\n",
    " (234, 1),\n",
    " (236, 1),\n",
    " (237, 12),\n",
    " (238, 1),\n",
    " (239, 1),\n",
    " (240, 2),\n",
    " (244, 1),\n",
    " (252, 14),\n",
    " (255, 5),\n",
    " (257, 5),\n",
    " (258, 2),\n",
    " (260, 1),\n",
    " (263, 1),\n",
    " (268, 2),\n",
    " (270, 2),\n",
    " (273, 1),\n",
    " (280, 2),\n",
    " (286, 1),\n",
    " (289, 1),\n",
    " (290, 3),\n",
    " (291, 2),\n",
    " (293, 1),\n",
    " (297, 1),\n",
    " (298, 3),\n",
    " (300, 2),\n",
    " (303, 3),\n",
    " (307, 1),\n",
    " (309, 1),\n",
    " (310, 1),\n",
    " (316, 4),\n",
    " (324, 7),\n",
    " (330, 2),\n",
    " (331, 1),\n",
    " (332, 1),\n",
    " (333, 2),\n",
    " (335, 1),\n",
    " (336, 4),\n",
    " (337, 3),\n",
    " (339, 5),\n",
    " (343, 2),\n",
    " (345, 1),\n",
    " (346, 6),\n",
    " (350, 17),\n",
    " (353, 1),\n",
    " (354, 5),\n",
    " (357, 4),\n",
    " (358, 4),\n",
    " (360, 3),\n",
    " (363, 1),\n",
    " (373, 3),\n",
    " (374, 2),\n",
    " (376, 10),\n",
    " (378, 5),\n",
    " (383, 1),\n",
    " (394, 6),\n",
    " (395, 1),\n",
    " (400, 1),\n",
    " (401, 1),\n",
    " (405, 4),\n",
    " (406, 2),\n",
    " (407, 1),\n",
    " (409, 3),\n",
    " (417, 4),\n",
    " (423, 1),\n",
    " (427, 5),\n",
    " (430, 2),\n",
    " (432, 2),\n",
    " (433, 1),\n",
    " (434, 3),\n",
    " (440, 1),\n",
    " (442, 1),\n",
    " (444, 6),\n",
    " (447, 1),\n",
    " (454, 1),\n",
    " (460, 15),\n",
    " (461, 8),\n",
    " (462, 3),\n",
    " (465, 1),\n",
    " (466, 1),\n",
    " (468, 2),\n",
    " (469, 1),\n",
    " (470, 1),\n",
    " (473, 2),\n",
    " (475, 4),\n",
    " (478, 1),\n",
    " (482, 2),\n",
    " (485, 1),\n",
    " (488, 1),\n",
    " (489, 2),\n",
    " (492, 2),\n",
    " (495, 1),\n",
    " (500, 1),\n",
    " (501, 2),\n",
    " (504, 3),\n",
    " (505, 1),\n",
    " (513, 1),\n",
    " (514, 2),\n",
    " (518, 3),\n",
    " (521, 19),\n",
    " (523, 11),\n",
    " (526, 2),\n",
    " (529, 3),\n",
    " (530, 3),\n",
    " (531, 1),\n",
    " (532, 4),\n",
    " (533, 1),\n",
    " (536, 5),\n",
    " (541, 1),\n",
    " (542, 5),\n",
    " (545, 2),\n",
    " (549, 1),\n",
    " (550, 2),\n",
    " (551, 5),\n",
    " (555, 3),\n",
    " (556, 1),\n",
    " (559, 1),\n",
    " (561, 3),\n",
    " (567, 3),\n",
    " (570, 1),\n",
    " (574, 1),\n",
    " (576, 1),\n",
    " (582, 1),\n",
    " (585, 1),\n",
    " (586, 1),\n",
    " (587, 15),\n",
    " (588, 1),\n",
    " (589, 1),\n",
    " (590, 3),\n",
    " (593, 2),\n",
    " (607, 1),\n",
    " (608, 2),\n",
    " (616, 2),\n",
    " (622, 3),\n",
    " (623, 1),\n",
    " (624, 1),\n",
    " (625, 1),\n",
    " (626, 8),\n",
    " (630, 1),\n",
    " (634, 1),\n",
    " (635, 4),\n",
    " (640, 1),\n",
    " (641, 3),\n",
    " (645, 1),\n",
    " (648, 2),\n",
    " (650, 5),\n",
    " (654, 1),\n",
    " (659, 1),\n",
    " (661, 1),\n",
    " (662, 2),\n",
    " (663, 1),\n",
    " (665, 4),\n",
    " (667, 1),\n",
    " (675, 2),\n",
    " (678, 1),\n",
    " (682, 2),\n",
    " (684, 1),\n",
    " (685, 1),\n",
    " (686, 2),\n",
    " (694, 1),\n",
    " (698, 1),\n",
    " (703, 1),\n",
    " (708, 2),\n",
    " (717, 1),\n",
    " (724, 2),\n",
    " (731, 7),\n",
    " (733, 3),\n",
    " (734, 4),\n",
    " (751, 6),\n",
    " (757, 4),\n",
    " (758, 2),\n",
    " (759, 11),\n",
    " (761, 5),\n",
    " (764, 13),\n",
    " (773, 1),\n",
    " (777, 2),\n",
    " (783, 1),\n",
    " (789, 5),\n",
    " (790, 2),\n",
    " (791, 2),\n",
    " (793, 1),\n",
    " (796, 1),\n",
    " (809, 1),\n",
    " (814, 6),\n",
    " (817, 2),\n",
    " (818, 5),\n",
    " (821, 1),\n",
    " (830, 2),\n",
    " (831, 15),\n",
    " (832, 3),\n",
    " (837, 8),\n",
    " (840, 1),\n",
    " (841, 4),\n",
    " (842, 1),\n",
    " (843, 3),\n",
    " (844, 13),\n",
    " (849, 8),\n",
    " (852, 20),\n",
    " (855, 2),\n",
    " (856, 8),\n",
    " (857, 8),\n",
    " (858, 1),\n",
    " (859, 1),\n",
    " (862, 3),\n",
    " (863, 1),\n",
    " (865, 1),\n",
    " (866, 3),\n",
    " (871, 11),\n",
    " (875, 2),\n",
    " (878, 2),\n",
    " (883, 3),\n",
    " (886, 1),\n",
    " (890, 1),\n",
    " (891, 1),\n",
    " (892, 2),\n",
    " (896, 4),\n",
    " (900, 2),\n",
    " (903, 1),\n",
    " (907, 2),\n",
    " (912, 3),\n",
    " (916, 3),\n",
    " (924, 13),\n",
    " (925, 1),\n",
    " (926, 1),\n",
    " (929, 2),\n",
    " (931, 2),\n",
    " (932, 6),\n",
    " (935, 1),\n",
    " (936, 1),\n",
    " (941, 2),\n",
    " (942, 4),\n",
    " (947, 1),\n",
    " (948, 1),\n",
    " (954, 3),\n",
    " (955, 2),\n",
    " (957, 88),\n",
    " (958, 5),\n",
    " (964, 3),\n",
    " (967, 4),\n",
    " (968, 2),\n",
    " (970, 4),\n",
    " (973, 3),\n",
    " (977, 1),\n",
    " (979, 1),\n",
    " (980, 2),\n",
    " (981, 1),\n",
    " (985, 4),\n",
    " (990, 11),\n",
    " (993, 2),\n",
    " (994, 1),\n",
    " (995, 1),\n",
    " (996, 1),\n",
    " (998, 2),\n",
    " (999, 10),\n",
    " (1001, 1),\n",
    " (1003, 4),\n",
    " (1006, 1),\n",
    " (1007, 1),\n",
    " (1010, 3),\n",
    " (1011, 1),\n",
    " (1014, 1),\n",
    " (1015, 9),\n",
    " (1016, 5),\n",
    " (1017, 2),\n",
    " (1020, 5),\n",
    " (1025, 2),\n",
    " (1027, 1),\n",
    " (1028, 17),\n",
    " (1030, 1),\n",
    " (1031, 1),\n",
    " (1034, 3),\n",
    " (1035, 2),\n",
    " (1037, 20),\n",
    " (1039, 3),\n",
    " (1040, 3),\n",
    " (1051, 1),\n",
    " (1052, 1),\n",
    " (1053, 2),\n",
    " (1062, 1),\n",
    " (1077, 1),\n",
    " (1085, 71),\n",
    " (1096, 3),\n",
    " (1108, 1),\n",
    " (1112, 1),\n",
    " (1113, 1),\n",
    " (1115, 1),\n",
    " (1121, 1),\n",
    " (1122, 1),\n",
    " (1124, 7),\n",
    " (1128, 1),\n",
    " (1132, 1),\n",
    " (1152, 3),\n",
    " (1153, 1),\n",
    " (1158, 1),\n",
    " (1161, 1),\n",
    " (1174, 1),\n",
    " (1177, 3),\n",
    " (1180, 1),\n",
    " (1184, 1),\n",
    " (1187, 6),\n",
    " (1195, 1),\n",
    " (1201, 1),\n",
    " (1209, 3),\n",
    " (1211, 1),\n",
    " (1216, 1),\n",
    " (1220, 1),\n",
    " (1221, 6),\n",
    " (1222, 1),\n",
    " (1235, 6),\n",
    " (1237, 1),\n",
    " (1240, 1),\n",
    " (1260, 3),\n",
    " (1268, 4),\n",
    " (1272, 1),\n",
    " (1274, 2),\n",
    " (1277, 1),\n",
    " (1279, 1),\n",
    " (1280, 2),\n",
    " (1282, 1),\n",
    " (1283, 3),\n",
    " (1300, 1),\n",
    " (1302, 1),\n",
    " (1303, 1),\n",
    " (1305, 1),\n",
    " (1312, 1),\n",
    " (1318, 1),\n",
    " (1319, 2),\n",
    " (1326, 1),\n",
    " (1327, 2),\n",
    " (1329, 1),\n",
    " (1331, 1),\n",
    " (1338, 2),\n",
    " (1351, 1),\n",
    " (1354, 1),\n",
    " (1356, 1),\n",
    " (1357, 4),\n",
    " (1360, 3),\n",
    " (1364, 1),\n",
    " (1372, 1),\n",
    " (1383, 1),\n",
    " (1386, 1),\n",
    " (1396, 1),\n",
    " (1402, 5),\n",
    " (1403, 3),\n",
    " (1411, 3),\n",
    " (1412, 1),\n",
    " (1413, 1),\n",
    " (1429, 1),\n",
    " (1433, 1),\n",
    " (1438, 3),\n",
    " (1440, 9),\n",
    " (1448, 1),\n",
    " (1455, 1),\n",
    " (1457, 1),\n",
    " (1459, 4),\n",
    " (1463, 3),\n",
    " (1464, 1),\n",
    " (1465, 5),\n",
    " (1475, 2),\n",
    " (1477, 3),\n",
    " (1478, 2),\n",
    " (1484, 1),\n",
    " (1487, 3),\n",
    " (1489, 18),\n",
    " (1495, 1),\n",
    " (1501, 1),\n",
    " (1509, 1),\n",
    " (1511, 2),\n",
    " (1514, 1),\n",
    " (1523, 4),\n",
    " (1528, 5),\n",
    " (1540, 2),\n",
    " (1548, 3),\n",
    " (1552, 2),\n",
    " (1563, 1),\n",
    " (1564, 2),\n",
    " (1566, 1),\n",
    " (1568, 1),\n",
    " (1576, 1),\n",
    " (1583, 1),\n",
    " (1593, 1),\n",
    " (1595, 3),\n",
    " (1598, 2),\n",
    " (1603, 3),\n",
    " (1604, 1),\n",
    " (1613, 4),\n",
    " (1621, 1),\n",
    " (1632, 2),\n",
    " (1634, 3),\n",
    " (1645, 1),\n",
    " (1652, 1),\n",
    " (1653, 1),\n",
    " (1656, 1),\n",
    " (1658, 1),\n",
    " (1660, 3),\n",
    " (1666, 1),\n",
    " (1669, 1),\n",
    " (1673, 3),\n",
    " (1675, 1),\n",
    " (1682, 1),\n",
    " (1694, 1),\n",
    " (1705, 1),\n",
    " (1719, 1),\n",
    " (1724, 1),\n",
    " (1770, 1),\n",
    " (1773, 1),\n",
    " (1775, 1),\n",
    " (1783, 5),\n",
    " (1787, 1),\n",
    " (1788, 1),\n",
    " (1794, 1),\n",
    " (1800, 1),\n",
    " (1806, 1),\n",
    " (1817, 1),\n",
    " (1821, 2),\n",
    " (1826, 1),\n",
    " (1828, 1),\n",
    " (1829, 1),\n",
    " (1835, 1),\n",
    " (1837, 1),\n",
    " (1838, 1),\n",
    " (1853, 1),\n",
    " (1857, 1),\n",
    " (1867, 3),\n",
    " (1874, 2),\n",
    " (1878, 9),\n",
    " (1881, 1),\n",
    " (1896, 1),\n",
    " (1897, 1),\n",
    " (1905, 1),\n",
    " (1908, 1),\n",
    " (1920, 2),\n",
    " (1922, 1),\n",
    " (1923, 2),\n",
    " (1924, 2),\n",
    " (1926, 6),\n",
    " (1928, 1),\n",
    " (1940, 1),\n",
    " (1944, 1),\n",
    " (1950, 1),\n",
    " (1959, 1),\n",
    " (1962, 2),\n",
    " (1965, 2),\n",
    " (1968, 1),\n",
    " (1969, 1),\n",
    " (1977, 2),\n",
    " (1988, 1),\n",
    " (1989, 1),\n",
    " (1994, 1),\n",
    " (2013, 2),\n",
    " (2022, 2),\n",
    " (2032, 2),\n",
    " (2033, 2),\n",
    " (2058, 2),\n",
    " (2082, 2),\n",
    " (2086, 4),\n",
    " (2104, 1),\n",
    " (2115, 6),\n",
    " (2116, 1),\n",
    " (2130, 1),\n",
    " (2138, 1),\n",
    " (2139, 2),\n",
    " (2159, 1),\n",
    " (2164, 6),\n",
    " (2166, 1),\n",
    " (2175, 1),\n",
    " (2200, 1),\n",
    " (2207, 2),\n",
    " (2209, 3),\n",
    " (2218, 1),\n",
    " (2232, 1),\n",
    " (2235, 1),\n",
    " (2239, 3),\n",
    " (2241, 2),\n",
    " (2278, 4),\n",
    " (2294, 1),\n",
    " (2295, 5),\n",
    " (2303, 1),\n",
    " (2304, 3),\n",
    " (2308, 2),\n",
    " (2319, 1),\n",
    " (2328, 4),\n",
    " (2335, 1),\n",
    " (2338, 1),\n",
    " (2342, 1),\n",
    " (2345, 1),\n",
    " (2363, 2),\n",
    " (2369, 1),\n",
    " (2378, 1),\n",
    " (2387, 1),\n",
    " (2399, 5),\n",
    " (2424, 1),\n",
    " (2430, 2),\n",
    " (2432, 1),\n",
    " (2436, 1),\n",
    " (2437, 3),\n",
    " (2454, 5),\n",
    " (2463, 1),\n",
    " (2468, 1),\n",
    " (2476, 1),\n",
    " (2482, 1),\n",
    " (2485, 1),\n",
    " (2497, 1),\n",
    " (2498, 6),\n",
    " (2504, 8),\n",
    " (2535, 2),\n",
    " (2539, 1),\n",
    " (2544, 1),\n",
    " (2550, 2),\n",
    " (2551, 1),\n",
    " (2555, 1),\n",
    " (2557, 3),\n",
    " (2563, 1),\n",
    " (2573, 3),\n",
    " (2610, 1),\n",
    " (2611, 2),\n",
    " (2617, 1),\n",
    " (2622, 1),\n",
    " (2634, 1),\n",
    " (2649, 2),\n",
    " (2653, 1),\n",
    " (2655, 1),\n",
    " (2661, 1),\n",
    " (2673, 1),\n",
    " (2676, 2),\n",
    " (2680, 1),\n",
    " (2681, 7),\n",
    " (2699, 2),\n",
    " (2706, 1),\n",
    " (2721, 1),\n",
    " (2744, 1),\n",
    " (2747, 1),\n",
    " (2748, 1),\n",
    " (2752, 1),\n",
    " (2764, 1),\n",
    " (2770, 2),\n",
    " (2778, 1),\n",
    " (2781, 1),\n",
    " (2785, 3),\n",
    " (2812, 1),\n",
    " (2816, 11),\n",
    " (2821, 1),\n",
    " (2822, 2),\n",
    " (2839, 1),\n",
    " (2840, 1),\n",
    " (2844, 1),\n",
    " (2858, 1),\n",
    " (2863, 4),\n",
    " (2871, 1),\n",
    " (2880, 1),\n",
    " (2896, 2),\n",
    " (2908, 1),\n",
    " (2910, 1),\n",
    " (2919, 1),\n",
    " (2930, 1),\n",
    " (2937, 1),\n",
    " (2975, 4),\n",
    " (2977, 1),\n",
    " (2983, 1),\n",
    " (2985, 1),\n",
    " (3005, 1),\n",
    " (3015, 2),\n",
    " (3019, 2),\n",
    " (3026, 1),\n",
    " (3028, 1),\n",
    " (3029, 1),\n",
    " (3033, 1),\n",
    " (3038, 1),\n",
    " (3047, 1),\n",
    " (3059, 1),\n",
    " (3070, 1),\n",
    " (3076, 2),\n",
    " (3085, 6),\n",
    " (3102, 1),\n",
    " (3109, 3),\n",
    " (3112, 2),\n",
    " (3116, 2),\n",
    " (3142, 1),\n",
    " (3154, 1),\n",
    " (3171, 1),\n",
    " (3178, 2),\n",
    " (3191, 1),\n",
    " (3193, 1),\n",
    " (3195, 2),\n",
    " (3207, 5),\n",
    " (3254, 1),\n",
    " (3273, 1),\n",
    " (3274, 1),\n",
    " (3278, 3),\n",
    " (3280, 3),\n",
    " (3290, 1),\n",
    " (3291, 1),\n",
    " (3292, 1),\n",
    " (3293, 1),\n",
    " (3294, 1),\n",
    " (3295, 1),\n",
    " (3296, 2),\n",
    " (3297, 1),\n",
    " (3298, 2),\n",
    " (3299, 1),\n",
    " (3300, 1),\n",
    " (3301, 1),\n",
    " (3302, 1),\n",
    " (3303, 1),\n",
    " (3304, 2),\n",
    " (3305, 3),\n",
    " (3306, 1),\n",
    " (3307, 1),\n",
    " (3308, 1),\n",
    " (3309, 4),\n",
    " (3310, 1),\n",
    " (3311, 1),\n",
    " (3312, 1),\n",
    " (3313, 1),\n",
    " (3314, 3),\n",
    " (3315, 1),\n",
    " (3316, 1),\n",
    " (3317, 1),\n",
    " (3318, 1),\n",
    " (3319, 1),\n",
    " (3320, 1),\n",
    " (3321, 3),\n",
    " (3322, 1),\n",
    " (3323, 2),\n",
    " (3324, 1),\n",
    " (3325, 1),\n",
    " (3326, 1),\n",
    " (3327, 1),\n",
    " (3328, 1),\n",
    " (3329, 1),\n",
    " (3330, 1),\n",
    " (3331, 2),\n",
    " (3332, 1),\n",
    " (3333, 1),\n",
    " (3334, 1),\n",
    " (3335, 1),\n",
    " (3336, 1),\n",
    " (3337, 1),\n",
    " (3338, 1),\n",
    " (3339, 1),\n",
    " (3340, 1),\n",
    " (3341, 3),\n",
    " (3342, 1),\n",
    " (3343, 1),\n",
    " (3344, 2),\n",
    " (3345, 1),\n",
    " (3346, 1),\n",
    " (3347, 1),\n",
    " (3348, 1),\n",
    " (3349, 2),\n",
    " (3350, 2),\n",
    " (3351, 1),\n",
    " (3352, 1),\n",
    " (3353, 1),\n",
    " (3354, 2),\n",
    " (3355, 1),\n",
    " (3356, 1),\n",
    " (3357, 6),\n",
    " (3358, 1),\n",
    " (3359, 1),\n",
    " (3360, 2),\n",
    " (3361, 1),\n",
    " (3362, 2),\n",
    " (3363, 2),\n",
    " (3364, 1),\n",
    " (3365, 1),\n",
    " (3366, 1),\n",
    " (3367, 2),\n",
    " (3368, 1),\n",
    " (3369, 1),\n",
    " (3370, 7),\n",
    " (3371, 1),\n",
    " (3372, 1),\n",
    " (3373, 1),\n",
    " (3374, 1),\n",
    " (3375, 2),\n",
    " (3376, 1),\n",
    " (3377, 1),\n",
    " (3378, 1),\n",
    " (3379, 1),\n",
    " (3380, 1),\n",
    " (3381, 2),\n",
    " (3382, 1),\n",
    " (3383, 1),\n",
    " (3384, 1),\n",
    " (3385, 1),\n",
    " (3386, 1),\n",
    " (3387, 1),\n",
    " (3388, 1),\n",
    " (3389, 1),\n",
    " (3390, 1),\n",
    " (3391, 1),\n",
    " (3392, 1),\n",
    " (3393, 1),\n",
    " (3394, 4),\n",
    " (3395, 1),\n",
    " (3396, 2),\n",
    " (3397, 1),\n",
    " (3398, 1),\n",
    " (3399, 1),\n",
    " (3400, 2),\n",
    " (3401, 1),\n",
    " (3402, 1),\n",
    " (3403, 1),\n",
    " (3404, 1),\n",
    " (3405, 1),\n",
    " (3406, 2),\n",
    " (3407, 1),\n",
    " (3408, 1),\n",
    " (3409, 1),\n",
    " (3410, 1),\n",
    " (3411, 1),\n",
    " (3412, 1),\n",
    " (3413, 1),\n",
    " (3414, 1),\n",
    " (3415, 1),\n",
    " (3416, 1),\n",
    " (3417, 1),\n",
    " (3418, 2),\n",
    " (3419, 1),\n",
    " (3420, 1),\n",
    " (3421, 1),\n",
    " (3422, 1),\n",
    " (3423, 1),\n",
    " (3424, 1),\n",
    " (3425, 1),\n",
    " (3426, 1),\n",
    " (3427, 1),\n",
    " (3428, 1),\n",
    " (3429, 1),\n",
    " (3430, 1),\n",
    " (3431, 1),\n",
    " (3432, 1),\n",
    " (3433, 1),\n",
    " (3434, 1),\n",
    " (3435, 1),\n",
    " (3436, 7),\n",
    " (3437, 1),\n",
    " (3438, 1),\n",
    " (3439, 1),\n",
    " (3440, 1),\n",
    " (3441, 1),\n",
    " (3442, 1),\n",
    " (3443, 1),\n",
    " (3444, 4),\n",
    " (3445, 2),\n",
    " (3446, 1),\n",
    " (3447, 2),\n",
    " (3448, 1),\n",
    " (3449, 2),\n",
    " (3450, 1),\n",
    " (3451, 1),\n",
    " (3452, 1),\n",
    " (3453, 1),\n",
    " (3454, 1),\n",
    " (3455, 1),\n",
    " (3456, 1),\n",
    " (3457, 1),\n",
    " (3458, 1),\n",
    " (3459, 1),\n",
    " (3460, 1),\n",
    " (3461, 1),\n",
    " (3462, 1),\n",
    " (3463, 1),\n",
    " (3464, 1),\n",
    " (3465, 1),\n",
    " (3466, 1),\n",
    " (3467, 1),\n",
    " (3468, 2),\n",
    " (3469, 1),\n",
    " (3470, 1),\n",
    " (3471, 5),\n",
    " (3472, 1),\n",
    " (3473, 1),\n",
    " (3474, 1),\n",
    " (3475, 1),\n",
    " (3476, 3),\n",
    " (3477, 1),\n",
    " (3478, 1),\n",
    " (3479, 1),\n",
    " (3480, 1),\n",
    " (3481, 1),\n",
    " (3482, 1),\n",
    " (3483, 1),\n",
    " (3484, 1),\n",
    " (3485, 1),\n",
    " (3486, 5),\n",
    " (3487, 1),\n",
    " (3488, 1),\n",
    " (3489, 1),\n",
    " (3490, 1),\n",
    " (3491, 1),\n",
    " (3492, 1),\n",
    " (3493, 1),\n",
    " (3494, 1),\n",
    " (3495, 1),\n",
    " (3496, 2),\n",
    " (3497, 1),\n",
    " (3498, 1),\n",
    " (3499, 10),\n",
    " (3500, 1),\n",
    " (3501, 2),\n",
    " (3502, 1),\n",
    " (3503, 1),\n",
    " (3504, 1),\n",
    " (3505, 2),\n",
    " (3506, 1),\n",
    " (3507, 8),\n",
    " (3508, 1),\n",
    " (3509, 2),\n",
    " (3510, 1),\n",
    " (3511, 1),\n",
    " (3512, 1),\n",
    " (3513, 1),\n",
    " (3514, 2),\n",
    " (3515, 1),\n",
    " (3516, 1),\n",
    " (3517, 2),\n",
    " (3518, 1),\n",
    " (3519, 1),\n",
    " (3520, 1),\n",
    " (3521, 1),\n",
    " (3522, 1),\n",
    " (3523, 1),\n",
    " (3524, 1),\n",
    " (3525, 1),\n",
    " (3526, 1),\n",
    " (3527, 1),\n",
    " (3528, 7),\n",
    " (3529, 1),\n",
    " (3530, 1),\n",
    " (3531, 1),\n",
    " (3532, 1),\n",
    " (3533, 1),\n",
    " (3534, 1),\n",
    " (3535, 1),\n",
    " (3536, 1),\n",
    " (3537, 1),\n",
    " (3538, 2),\n",
    " (3539, 1),\n",
    " (3540, 1),\n",
    " (3541, 1),\n",
    " (3542, 1),\n",
    " (3543, 2),\n",
    " (3544, 1),\n",
    " (3545, 1),\n",
    " (3546, 1),\n",
    " (3547, 4),\n",
    " (3548, 4),\n",
    " (3549, 2),\n",
    " (3550, 1),\n",
    " (3551, 1),\n",
    " (3552, 1),\n",
    " (3553, 1),\n",
    " (3554, 1),\n",
    " (3555, 1),\n",
    " (3556, 1),\n",
    " (3557, 1),\n",
    " (3558, 2),\n",
    " (3559, 1),\n",
    " (3560, 1),\n",
    " (3561, 2),\n",
    " (3562, 1),\n",
    " (3563, 1),\n",
    " (3564, 2),\n",
    " (3565, 1),\n",
    " (3566, 1),\n",
    " (3567, 2),\n",
    " (3568, 2),\n",
    " (3569, 1),\n",
    " (3570, 1),\n",
    " (3571, 1),\n",
    " (3572, 1),\n",
    " (3573, 1),\n",
    " (3574, 1),\n",
    " (3575, 1),\n",
    " (3576, 1),\n",
    " (3577, 1),\n",
    " (3578, 1),\n",
    " (3579, 3),\n",
    " (3580, 1),\n",
    " (3581, 1),\n",
    " (3582, 2),\n",
    " (3583, 1),\n",
    " (3584, 3),\n",
    " (3585, 1),\n",
    " (3586, 1),\n",
    " (3587, 1),\n",
    " (3588, 1),\n",
    " (3589, 1),\n",
    " (3590, 2),\n",
    " (3591, 3),\n",
    " (3592, 1),\n",
    " (3593, 1),\n",
    " (3594, 1),\n",
    " (3595, 5),\n",
    " (3596, 1),\n",
    " (3597, 1),\n",
    " (3598, 5),\n",
    " (3599, 1),\n",
    " (3600, 1),\n",
    " (3601, 2),\n",
    " (3602, 1),\n",
    " (3603, 1),\n",
    " (3604, 1),\n",
    " (3605, 1),\n",
    " (3606, 1),\n",
    " (3607, 2),\n",
    " (3608, 4),\n",
    " (3609, 1),\n",
    " (3610, 2),\n",
    " (3611, 1),\n",
    " (3612, 1),\n",
    " (3613, 2),\n",
    " (3614, 1),\n",
    " (3615, 1),\n",
    " (3616, 1),\n",
    " (3617, 2),\n",
    " (3618, 1),\n",
    " (3619, 1),\n",
    " (3620, 1),\n",
    " (3621, 2),\n",
    " (3622, 1),\n",
    " (3623, 3),\n",
    " (3624, 1),\n",
    " (3625, 1),\n",
    " (3626, 1),\n",
    " (3627, 1),\n",
    " (3628, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TfidfModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple topic identification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of each entity\n",
    "print(type(ent))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print the entities\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'MÃ¡rquez' or 'Gabo'\n",
    "    if \"MÃ¡rquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count * 1.0 / len(txt.entities)\n",
    "print(percentage)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a set of spaCy entities keeping only their text: spacy_ents\n",
    "spacy_ents = {e.text for e in doc.ents} \n",
    "\n",
    "# Create a set of the intersection between the spacy and polyglot entities: ensemble_ents\n",
    "ensemble_ents = spacy_ents.intersection(poly_ents)\n",
    "\n",
    "# Print the common entities\n",
    "print(ensemble_ents)\n",
    "\n",
    "# Calculate the number of entities not included in the new ensemble set of entities: num_left_out\n",
    "num_left_out = len(spacy_ents.union(poly_ents)) - len(ensemble_ents)\n",
    "print(num_left_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a \"fake news\" classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
